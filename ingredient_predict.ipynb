{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    '''Import cleaned recipe, raw recipe and ratings data'''\n",
    "    #cleaned recipes\n",
    "    recipes = pd.read_csv('PP_recipes.csv')\n",
    "    del recipes['i']\n",
    "    del recipes['name_tokens']\n",
    "    del recipes['ingredient_tokens']\n",
    "    del recipes['steps_tokens']\n",
    "    recipes = recipes.set_index('id')\n",
    "\n",
    "    #ratings\n",
    "    ratings = pd.read_csv('RAW_interactions.csv')\n",
    "    del ratings['user_id']\n",
    "    del ratings['date']\n",
    "    del ratings['review']\n",
    "    ratings = ratings.set_index('recipe_id')\n",
    "\n",
    "    #raw recipe info\n",
    "    raw_recipes = pd.read_csv('RAW_recipes.csv')\n",
    "    del raw_recipes['contributor_id']\n",
    "    del raw_recipes['submitted']\n",
    "    del raw_recipes['tags']\n",
    "    del raw_recipes['steps']\n",
    "    del raw_recipes['description']\n",
    "    raw_recipes = raw_recipes.set_index('id')\n",
    "\n",
    "    return (recipes, raw_recipes, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>minutes</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137739</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>55</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31490</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>30</td>\n",
       "      <td>[173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112140</th>\n",
       "      <td>all in the kitchen  chili</td>\n",
       "      <td>130</td>\n",
       "      <td>[269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>['ground beef', 'yellow onions', 'diced tomato...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59389</th>\n",
       "      <td>alouette  potatoes</td>\n",
       "      <td>45</td>\n",
       "      <td>[368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['spreadable cheese with garlic and herbs', 'n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44061</th>\n",
       "      <td>amish  tomato ketchup  for canning</td>\n",
       "      <td>190</td>\n",
       "      <td>[352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486161</th>\n",
       "      <td>zydeco soup</td>\n",
       "      <td>60</td>\n",
       "      <td>[415.2, 26.0, 34.0, 26.0, 44.0, 21.0, 15.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>['celery', 'onion', 'green sweet pepper', 'gar...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493372</th>\n",
       "      <td>zydeco spice mix</td>\n",
       "      <td>5</td>\n",
       "      <td>[14.8, 0.0, 2.0, 58.0, 1.0, 0.0, 1.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>['paprika', 'salt', 'garlic powder', 'onion po...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308080</th>\n",
       "      <td>zydeco ya ya deviled eggs</td>\n",
       "      <td>40</td>\n",
       "      <td>[59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>['hard-cooked eggs', 'mayonnaise', 'dijon must...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298512</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>29</td>\n",
       "      <td>[188.0, 11.0, 57.0, 11.0, 7.0, 21.0, 9.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['butter', 'eagle brand condensed milk', 'ligh...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298509</th>\n",
       "      <td>cookies by design   sugar shortbread cookies</td>\n",
       "      <td>20</td>\n",
       "      <td>[174.9, 14.0, 33.0, 4.0, 4.0, 11.0, 6.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>['granulated sugar', 'shortening', 'eggs', 'fl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231637 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  minutes  \\\n",
       "id                                                              \n",
       "137739    arriba   baked winter squash mexican style       55   \n",
       "31490               a bit different  breakfast pizza       30   \n",
       "112140                     all in the kitchen  chili      130   \n",
       "59389                             alouette  potatoes       45   \n",
       "44061             amish  tomato ketchup  for canning      190   \n",
       "...                                              ...      ...   \n",
       "486161                                   zydeco soup       60   \n",
       "493372                              zydeco spice mix        5   \n",
       "308080                     zydeco ya ya deviled eggs       40   \n",
       "298512        cookies by design   cookies on a stick       29   \n",
       "298509  cookies by design   sugar shortbread cookies       20   \n",
       "\n",
       "                                          nutrition  n_steps  \\\n",
       "id                                                             \n",
       "137739        [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "31490     [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
       "112140   [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]        6   \n",
       "59389     [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]       11   \n",
       "44061     [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]        5   \n",
       "...                                             ...      ...   \n",
       "486161  [415.2, 26.0, 34.0, 26.0, 44.0, 21.0, 15.0]        7   \n",
       "493372        [14.8, 0.0, 2.0, 58.0, 1.0, 0.0, 1.0]        1   \n",
       "308080         [59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]        7   \n",
       "298512    [188.0, 11.0, 57.0, 11.0, 7.0, 21.0, 9.0]        9   \n",
       "298509     [174.9, 14.0, 33.0, 4.0, 4.0, 11.0, 6.0]        5   \n",
       "\n",
       "                                              ingredients  n_ingredients  \n",
       "id                                                                        \n",
       "137739  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
       "31490   ['prepared pizza crust', 'sausage patty', 'egg...              6  \n",
       "112140  ['ground beef', 'yellow onions', 'diced tomato...             13  \n",
       "59389   ['spreadable cheese with garlic and herbs', 'n...             11  \n",
       "44061   ['tomato juice', 'apple cider vinegar', 'sugar...              8  \n",
       "...                                                   ...            ...  \n",
       "486161  ['celery', 'onion', 'green sweet pepper', 'gar...             22  \n",
       "493372  ['paprika', 'salt', 'garlic powder', 'onion po...             13  \n",
       "308080  ['hard-cooked eggs', 'mayonnaise', 'dijon must...              8  \n",
       "298512  ['butter', 'eagle brand condensed milk', 'ligh...             10  \n",
       "298509  ['granulated sugar', 'shortening', 'eggs', 'fl...              7  \n",
       "\n",
       "[231637 rows x 6 columns]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,raw_recipes,ratings = import_data()\n",
    "raw_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(s):\n",
    "    '''Converts a string that is formatted like a list to a list'''\n",
    "    l = re.findall(r'\\w[\\w\\s]+',s)\n",
    "    #l = ' '.join(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['winter squash',\n",
       "  'mexican seasoning',\n",
       "  'mixed spice',\n",
       "  'honey',\n",
       "  'butter',\n",
       "  'olive oil',\n",
       "  'salt']]"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning token lists\n",
    "input_sequences = raw_recipes['ingredients'].apply(string_to_list).to_list()\n",
    "input_sequences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengths = [len(x.split(' ')) for x in input_sequences]\n",
    "#sns.histplot(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(input_sequences):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(input_sequences)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    ngram_sequences = []\n",
    "    for line in input_sequences:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            ngram_sequences.append(n_gram_sequence)\n",
    "    return ngram_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[492], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_sequences,total_words \u001b[39m=\u001b[39m get_sequence_of_tokens(input_sequences)\n\u001b[1;32m      2\u001b[0m input_sequences[:\u001b[39m6\u001b[39m]\n",
      "Cell \u001b[0;32mIn[377], line 4\u001b[0m, in \u001b[0;36mget_sequence_of_tokens\u001b[0;34m(input_sequences)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sequence_of_tokens\u001b[39m(input_sequences):\n\u001b[1;32m      3\u001b[0m     \u001b[39m## tokenization\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     tokenizer\u001b[39m.\u001b[39;49mfit_on_texts(input_sequences)\n\u001b[1;32m      5\u001b[0m     total_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      7\u001b[0m     ngram_sequences \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Envs/food_env/lib/python3.10/site-packages/keras/src/preprocessing/text.py:293\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m         seq \u001b[39m=\u001b[39m text_to_word_sequence(\n\u001b[1;32m    294\u001b[0m             text,\n\u001b[1;32m    295\u001b[0m             filters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilters,\n\u001b[1;32m    296\u001b[0m             lower\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlower,\n\u001b[1;32m    297\u001b[0m             split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit,\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    299\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m         seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer(text)\n",
      "File \u001b[0;32m~/Envs/food_env/lib/python3.10/site-packages/keras/src/preprocessing/text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m    A list of words (or tokens).\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 74\u001b[0m     input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     76\u001b[0m translate_dict \u001b[39m=\u001b[39m {c: split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m filters}\n\u001b[1;32m     77\u001b[0m translate_map \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "input_sequences,total_words = get_sequence_of_tokens(input_sequences)\n",
    "input_sequences[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find max token length\n",
    "def find_max(sequences):\n",
    "    max_length = max(len(x) for x in sequences)\n",
    "    return max_length\n",
    "\n",
    "max_length = find_max(input_sequences)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences (built in function returns a recursion error)\n",
    "def pad_sequences(sequence):\n",
    "    padded_sequence = []\n",
    "    for sequence_in in sequence:\n",
    "        zeros = (max_length-len(sequence_in))\n",
    "        padded = [int(0) for zero in range(zeros)]\n",
    "        padded.extend(sequence_in)\n",
    "        padded_sequence.append(padded)\n",
    "    return padded_sequence\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making test and train sets \n",
    "input_sequences = np.array(input_sequences)\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_length, max_token):\n",
    "    input_len = max_length -1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_token, 10, input_length=input_len))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(max_token, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 43, 10)            149910    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 14991)             1514091   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1708401 (6.52 MB)\n",
      "Trainable params: 1708401 (6.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_length, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60886/60886 [==============================] - 740s 12ms/step - loss: 5.0623\n"
     ]
    }
   ],
   "source": [
    "test_size = None\n",
    "history = model.fit(predictors[:test_size], label[:test_size], epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_for_me(seed_text, model):\n",
    "    next_words = 5\n",
    "    words_out = []\n",
    "    while next_words > 0:\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]      \n",
    "        token_list = pad_sequences([token_list])\n",
    "        token_list = np.array(token_list)\n",
    "        token_list = token_list[:,1:]\n",
    "        proba = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(proba, axis=1)\n",
    "        word = list(tokenizer.word_index.keys())[int(predicted-1)]\n",
    "        words_out.append(word)\n",
    "        next_words -= 1\n",
    "        seed_text = seed_text + ' ' + word\n",
    "    words_out = list(set(words_out))\n",
    "    string_out = 'Why not try adding some '+', '.join(words_out[:-1])+' and '+words_out[-1]+'?'\n",
    "    return string_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_suggestions(ingredients_in_stock):\n",
    "    for ingredient in ingredients_in_stock:\n",
    "        split_ingredients = ' and '.join(ingredient.split(' '))\n",
    "        greeting = 'For '+split_ingredients.upper() +' let me see... '\n",
    "        print(greeting)\n",
    "        print(cook_for_me(ingredient,model))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For CHICKEN let me see... \n",
      "Why not try adding some olive oil, onion, garlic cloves, salt and pepper?\n",
      "\n",
      "For CHICKEN AND RICE let me see... \n",
      "Why not try adding some carrot, celery, onion and water?\n",
      "\n",
      "For TUNA let me see... \n",
      "Why not try adding some celery, mayonnaise, onion and salt?\n",
      "\n",
      "For CHOCOLATE AND SUGAR let me see... \n",
      "Why not try adding some flour, salt, milk, butter and eggs?\n",
      "\n",
      "For PINEAPPLE let me see... \n",
      "Why not try adding some ice, sugar, lemon juice and water?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingredients_in_stock = ['chicken','chicken rice','tuna','chocolate sugar','pineapple']\n",
    "make_suggestions(ingredients_in_stock)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food_env",
   "language": "python",
   "name": "food_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
