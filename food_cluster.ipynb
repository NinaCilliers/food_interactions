{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from scipy.stats import bayes_mvs\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from time import time\n",
    "import copy\n",
    "import pickle\n",
    "import missingno as msno\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>What is the Bayesian average?</b></h4>\n",
    "The Bayesian average uses two constants to offset the arithmetic average of an individual product. This is important as otherwise products with only one five star rating are affoarding the same quality ranking as products with thousands of five star reviews. To account for differences in certainty, the rating for recipes with less than a critical number of reviews are adjusted, whlile recipes above this threshold are only very slightly adjusted. The critical value (C) is the number of reviews of the 25% quartile, which for our dataset is 1. The formula for the Bayesian average (r bar) is shown below where r and c are the rating and rating count for an individual recipe and C and R are critical threshold and average rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "<h3>Importing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    #cleaned recipes\n",
    "    recipes = pd.read_csv('PP_recipes.csv')\n",
    "    del recipes['i']\n",
    "    del recipes['name_tokens']\n",
    "    del recipes['ingredient_tokens']\n",
    "    del recipes['steps_tokens']\n",
    "    recipes = recipes.set_index('id')\n",
    "\n",
    "    #ratings\n",
    "    ratings = pd.read_csv('RAW_interactions.csv')\n",
    "    del ratings['user_id']\n",
    "    del ratings['date']\n",
    "    del ratings['review']\n",
    "    ratings = ratings.set_index('recipe_id')\n",
    "\n",
    "    #raw recipe info\n",
    "    raw_recipes = pd.read_csv('RAW_recipes.csv')\n",
    "    del raw_recipes['contributor_id']\n",
    "    del raw_recipes['submitted']\n",
    "    del raw_recipes['tags']\n",
    "    del raw_recipes['steps']\n",
    "    del raw_recipes['description']\n",
    "    raw_recipes = raw_recipes.set_index('id')\n",
    "\n",
    "    return (recipes, raw_recipes, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_, raw_recipes_, ratings_ = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(raw_recipes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ratings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for outliers\n",
    "print(f'Count of ratings > 5: {(ratings > 5).sum()[0]}')\n",
    "print(f'Count of ratings < 0: {(ratings < 0).sum()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two methods for re-scaling reviews based on review number \n",
    "\n",
    "#bayesian ci\n",
    "def bayes_ci(n,rating,alpha=0.95):\n",
    "    mean,_,_ = bayes_mvs(n,alpha)\n",
    "    return mean.statistic\n",
    "\n",
    "#bayesian average\n",
    "avg = ratings.mean()\n",
    "count = ratings.count()\n",
    "C = (ratings.reset_index().groupby('recipe_id').agg('size').sort_values().quantile(.25))\n",
    "m = ratings.mean().rating\n",
    "\n",
    "def bayes_avg(n):\n",
    "    avg = n.mean()\n",
    "    count = n.count()\n",
    "    return (avg*count+C*m)/(count+C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_ = copy.copy(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ratings.quantile(0.95)\n",
    "C = ratings.mean()\n",
    "\n",
    "def weighted_rating(s, m=q, c=C):\n",
    "    R = s.mean()\n",
    "    v = s.count()\n",
    "    return ((v/(v+m) * R) + (m/(m+v) * C))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale ratings\n",
    "def scale_ratings(ratings):\n",
    "    ratings = ratings.reset_index()\n",
    "    ratings = ratings.groupby('recipe_id').agg({'rating':bayes_avg})\n",
    "    #ratings = ratings.reset_index()\n",
    "    #df = df.drop_duplicates(subset='recipe_id')\n",
    "    return ratings #df.set_index('recipe_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating reference dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = raw_recipes[['name','ingredients']]\n",
    "df_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Encoding and normalizing features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_column(df,key):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(np.array(df[key]).reshape(-1,1)),columns=[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = ['calorie_level','minutes','n_steps','n_ingredients','rating']\n",
    "\n",
    "def normalize_select_cols(df,cols_to_normalize):\n",
    "    for col in cols_to_normalize:\n",
    "        df[col] = scale_column(df,col).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating clustering dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to time fits\n",
    "def timer_func(func):\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging data into single usable df\n",
    "def merge_frames(recipes,raw_recipes, ratings):\n",
    "    return recipes.merge(raw_recipes[['minutes','nutrition','n_steps','n_ingredients']],how='inner',left_index=True,right_index=True).merge(ratings,how='inner',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string that looks lke a list of numbers to a list of \n",
    "def string_to_list(s):\n",
    "    l = re.findall(r'\\d+',s)\n",
    "    l = [int(num) for num in l]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of top ingredients \n",
    "def get_top(recipes=recipes,n_ingred=1000,show=False):\n",
    "    if isinstance(recipes['ingredient_ids'].iloc[0],str):\n",
    "        ingred = recipes['ingredient_ids'].apply(string_to_list)\n",
    "    else: ingred = recipes['ingredient_ids']\n",
    "    ingred = list(itertools.chain.from_iterable(ingred))\n",
    "    total_num_ingred = len(Counter(ingred))\n",
    "    if show:\n",
    "        print(f'Total ingredient number: {total_num_ingred}')\n",
    "    ingred_counts = Counter(ingred).most_common(n_ingred)\n",
    "    top_ingredients = [ingred_counts[n][0] for n in range(len(ingred_counts))]\n",
    "    return total_num_ingred, top_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_ingred,_ = get_top(recipes_,show=True)\n",
    "top_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering ingredients that aren't in the top ingredient list\n",
    "def trim_ingredients(recipes,size=None,top_ingredients=top_ingredients):\n",
    "    if not size:\n",
    "        size = recipes['ingredient_ids'].shape[0]\n",
    "        #print(size)\n",
    "\n",
    "    ingredient_list = deque([])\n",
    "\n",
    "    for n in range(size): #df[1].shape[0]):\n",
    "        test=string_to_list(recipes['ingredient_ids'].iloc[n])\n",
    "        selected = [i for i in test if i in top_ingredients]\n",
    "        ingredient_list.append(selected)\n",
    "    recipes = recipes.iloc[:size,:]\n",
    "    recipes['ingredient_ids']=ingredient_list\n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert one list column to encoded columns\n",
    "def transform_to_sparse(df,key,size,encoded=False,sparse=True):\n",
    "    if isinstance(df[key].iloc[0],str):\n",
    "        data = df[key].apply(string_to_list)\n",
    "    else: data = df[key]\n",
    "    t_names = [str(n)+' '+key for n in range(size)]\n",
    "    data = pd.DataFrame(data.to_list(), columns=t_names,index=df.index.values)\n",
    "    if encoded:\n",
    "        return data \n",
    "    else: \n",
    "        data = pd.get_dummies(data.stack(),sparse=sparse)\n",
    "        data = data.groupby(level=0).sum()\n",
    "        #data = data.fillna(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all keys to encoded columns\n",
    "def transform_all_to_sparse(df,keys,sizes,encoded=False,sparse=True):\n",
    "    #encode\n",
    "    for n,key in enumerate(keys):\n",
    "        data = transform_to_sparse(df,key,sizes[n],encoded[n],sparse[n])\n",
    "        yield data \n",
    "        #delete original columns\n",
    "        try:\n",
    "            df.pop(key)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all encoded columns into one matricies with other data\n",
    "def concat_df(g,df):\n",
    "    df_concat = pd.concat(g,axis=1)\n",
    "    df = pd.concat([df_concat,df],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data cleaning pipeline</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to do all \n",
    "@timer_func\n",
    "class CleanAndTransform():\n",
    "    def __init__(self,n_ingred=1000,test_size=None):\n",
    "        #data attributes\n",
    "        self.keys  = ['techniques','ingredient_ids','nutrition']\n",
    "        self.sizes = (58,19,14)\n",
    "        self.encoded = (True,False,False)\n",
    "        self.sparse = (False, False, False)\n",
    "        self.cols_to_normalize = ['calorie_level','minutes','n_steps','n_ingredients','rating']\n",
    "        \n",
    "        #contorl output dimensions\n",
    "        self.n_ingred = n_ingred\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        #clean data\n",
    "        self.recipes, self.raw_recipes, self.ratings = import_data()   \n",
    "        _,self.top_ingredients = get_top(recipes=self.recipes,n_ingred=n_ingred,show=False)\n",
    "        self.recipes = trim_ingredients(recipes=self.recipes,size=self.test_size,top_ingredients=self.top_ingredients)\n",
    "        self.ratings = scale_ratings(self.ratings)\n",
    "        self.df = merge_frames(self.recipes, self.raw_recipes, self.ratings)\n",
    "        self.df = normalize_select_cols(self.df,self.cols_to_normalize)\n",
    "        self.df = self.df.fillna(0)\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        #encode data\n",
    "        self.df = self.df[:self.test_size]\n",
    "        self.g = transform_all_to_sparse(self.df,self.keys,self.sizes,self.encoded,self.sparse)\n",
    "        df = concat_df(self.g,self.df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = CleanAndTransform(test_size=100)\n",
    "df = transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data.pkl','wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_func\n",
    "def to_sparse(df):\n",
    "    return csr_matrix(df.values)\n",
    "\n",
    "to_sparse(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dimensionality Reduction </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95,random_state=10)\n",
    "df.columns = df.columns.astype(str)\n",
    "X_pca = pca.fit_transform(df)\n",
    "print('PCA reduced dimensions from ', df.shape[1],' to ',X_pca.shape[1] ,' and preserved 95% of variance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(pca.n_components_), pca.explained_variance_ratio_,color='mediumseagreen')\n",
    "plt.xlabel('Principle component')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xticks(range(pca.n_components_));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food_env",
   "language": "python",
   "name": "food_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
